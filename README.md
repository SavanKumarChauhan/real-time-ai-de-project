# âš¡ Real-Time AI-Powered Data Engineering Pipeline with Databricks & GPT

This open-source project showcases a **real-time, AI-enhanced data pipeline** built using the **modern Lakehouse architecture** on **Azure Databricks**. It simulates both **batch and streaming data**, processes it through **Bronze â†’ Silver â†’ Gold layers**, integrates **OpenAIâ€™s GPT** to generate smart sales summaries, and finally renders everything in a live **Databricks dashboard**.

> ðŸš€ Perfect for showcasing Data Engineering, Streaming, GPT Integration, Secure Azure Mounting, and End-to-End Workflow Orchestration skills â€” just like what modern data teams at tech-forward companies need.

---

## ðŸ’¡ What You'll See in This Project
![Real-Time Dashboard](real_time_dashboard/dashboard.JPG)


- ðŸ” **Real-Time Streaming + Batch Ingestion** using Parquet & Delta formats
- ðŸ§¹ **Dirty Data Cleansing & Schema Enforcement**
- ðŸª™ **Bronze â†’ Silver â†’ Gold Lakehouse Layers** with structured processing
- ðŸ¤– **AI-Driven GPT Summaries** generated from Gold data
- ðŸ“Š **Real-Time Sales Dashboard** inside Databricks
- ðŸ” **Enterprise-Grade Security** with Azure Key Vault + Secret Scopes
- ðŸ“ˆ **Fully Orchestrated Pipeline** using Databricks Jobs + Delta Live Tables (DLT)

---

## ðŸ§± Project Architecture

This project follows a modern **Lakehouse + AI design pattern** built on Databricks and Azure. Here's the high-level architecture:

- ðŸ”„ **Data Simulation**: Generates both batch and streaming Parquet files
- ðŸ§ª **Ingestion Layer**:
  - Batch â†’ Ingested via PySpark notebooks to Silver layer
  - Streaming â†’ Ingested using **Delta Live Tables (DLT)** and stored in Silver
- ðŸ§¹ **Transformation Layer**: 
  - Cleansed, validated, enriched in Silver
  - Processed into Gold with analytics-ready columns
- ðŸ¤– **AI Insights Layer**: 
  - Uses **OpenAI GPT-3.5** to summarize Gold data into natural language insights
- ðŸ“Š **Visualization Layer**:
  - Real-time Databricks dashboard (sales trends, categories, GPT summary)
- ðŸ” **Security Layer**:
  - Azure Key Vault + Secret Scopes for secure mounting and key management
- âš™ï¸ **Orchestration**:
  - A Unified Databricks Job manages batch, streaming, AI, and dashboard end-to-end

---

### ðŸ“Œ Architecture Diagram

![Project Architecture](real_time_dashboard/ArchitectureDiagram.png)

---

## ðŸŒŸ Key Features

This project showcases the most in-demand, real-world features companies expect from modern data engineers:

### âš™ï¸ Hybrid Ingestion Layer
- ðŸ”„ Simulates **both batch and streaming data**
- â¬…ï¸ Batch data is ingested using PySpark and stored in Bronze (Parquet)
- ðŸŒŠ Streaming data is ingested using **Delta Live Tables (DLT)**

### ðŸ§¹ Multi-Layer Lakehouse Architecture (Bronze â†’ Silver â†’ Gold)
- ðŸ’¾ Raw â†’ Cleaned â†’ Enriched data flow using **Delta format**
- ðŸ§  Gold layer includes reporting-specific columns (total revenue, year/month/day)

### ðŸ§  AI-Driven Insight Generation
- ðŸ¤– Uses **OpenAI GPT-3.5** to auto-generate sales summaries from Gold layer
- ðŸ” API key is secured with **Azure Key Vault + Databricks Secret Scope**
- ðŸ’¬ Summaries saved to Delta table for real-time dashboard embedding

### ðŸ“Š Real-Time Analytics Dashboard
- ðŸ“‰ Databricks SQL dashboard with:
  - Revenue trends over time
  - Sales by category and store
  - Top 10 best-selling products
  - ðŸ§  GPT-generated sales summary box

### ðŸ” Enterprise-Grade Security
- ðŸ”‘ OAuth-based mounting via **Azure Key Vault**
- ðŸ§¾ Secret scopes hide all credentials and tokens

### ðŸ§© End-to-End Orchestration
- ðŸ” Unified **Databricks Job** executes entire workflow:
  - Batch ingestion
  - Streaming pipeline (DLT)
  - Silver â†’ Gold â†’ GPT â†’ Dashboard
- ðŸ”„ Can be **scheduled or triggered** on demand

---

## ðŸ“’ Notebooks Used in the Pipeline

Below are the core notebooks used in the **Unified Workflow Job** â€” each playing a specific role in ingestion, transformation, AI, and analytics.

| ðŸ““ Notebook | ðŸ“‚ Path | âš™ï¸ Description |
|------------|---------|----------------|
| `01_streaming_to_silver` | [auto_ingestion_layer/01_streaming_to_silver](real_time_ai_de_project/auto_ingestion_layer/01_streaming_to_silver.ipynb) | Streaming sales ingestion via **DLT**, saves to Silver layer |
| `01_batch_ingestion_bronze_to_silver` | [`03_data_ingestion/01_batch_ingestion_bronze_to_silver`](real_time_ai_de_project/03_data_ingestion/01_batch_ingestion_bronze_to_silver.ipynb) | Ingests and transforms batch data (products, stores, promos, sales) to Silver |
| `01_batch_and_stream_processing` | [`real_time_ai_de_project/01_batch_and_stream_processing`](real_time_ai_de_project/04_gold_layer_creation/01_batch_and_stream_processing.ipynb) | Joins batch & stream Silver data â†’ builds enriched Gold table |
| `02_gold_optimization_and_view` | [`04_gold_layer_creation/02_gold_optimization_and_view`](real_time_ai_de_project/04_gold_layer_creation/02_gold_optimization_and_view.ipynb) | Optimizes Gold layer and prepares SQL views |
| `01_gpt_summary_generator` | [`05_ai_insights_layer/01_gpt_summary_generator`](real_time_ai_de_project/05_ai_insights_layer/01_gpt_summary_generator.ipynb) | Generates GPT summary from Gold layer using OpenAI API |
| `03_realtime_dashboard` | [`analytics_layer/03_realtime_dashboard`](real_time_ai_de_project/analytics_layer/03_realtime_dashboard.ipynb) | Builds real-time Databricks dashboard (charts + GPT summary) |

---

### ðŸ› ï¸ Additional Notebook

| ðŸ““ Notebook | ðŸ“‚ Path | âš™ï¸ Description |
|------------|---------|----------------|
| `01_env_config` | [`real_time_ai_de_project/01_env_config`](real_time_ai_de_project/01_env_config.ipynb) | Mounts ADLS containers securely using **Azure Key Vault + OAuth** |

> ðŸ§ª **Note:** Other notebooks in the repo are for testing/debugging only â€” not part of the production pipeline.


---

## ðŸ” Secure Access with Azure Key Vault

This project follows security best practices by using **Azure Key Vault** + **Databricks Secret Scopes** to avoid hardcoding any secrets.

### âœ… What We Secured

| ðŸ”‘ Secret | ðŸ”’ Stored In | ðŸ” Used For |
|-----------|--------------|-------------|
| `adls-client-secret` | Azure Key Vault | Mounting ADLS Gen2 containers securely via OAuth |
| `open-ai-api-key` | Azure Key Vault | Authenticating GPT-3.5 API for AI summaries |

---

### ðŸ§­ How It Works

1. **Azure Key Vault** holds all sensitive secrets
2. A **Databricks Secret Scope** is configured to connect to the Key Vault
3. Secrets are retrieved securely in notebooks via:
```python   
dbutils.secrets.get(scope="adls_scope", key="adls-client-secret")

```
---

## ðŸ§© Workflow Orchestration: Databricks Jobs + DLT

This project is orchestrated using a **Unified Databricks Job** that coordinates batch, streaming, AI, and dashboard tasks in sequence â€” just like an enterprise-grade data platform.

### ðŸ“Œ Job: `Unified_Workflow_Job`
![Unified_Workflow_Job](azure_resources_ss/unified_workflow_job.JPG)

This job runs 6 key notebooks in order:

1. `DLT_streaming_data` â†’ Triggers DLT pipeline (`pl_streaming_data_to_silver`)
2. `task_ingest_batch_data` â†’ Ingests batch files to Silver
3. `batch_and_stream_processing` â†’ Joins batch + stream data â†’ writes to Gold
4. `optimize_gold_and_create_view` â†’ Optimizes Gold & builds view for SQL/GPT
5. `gpt_summary` â†’ Sends query results to GPT-3.5 and stores natural language insights
6. `update_dashboard` â†’ Final dashboard update using latest data + GPT summary

ðŸ“ You can find the job here:  
[`job_definitions/Unified_Workflow_Job`](job_definitions/unified_workflow_job.json)

---

### ðŸ” DLT Pipeline: `pl_streaming_data_to_silver`
![DLT Pipeline](dlt_pipeline/PL_DLT.JPG)

This Delta Live Tables (DLT) pipeline is responsible for:

- Auto-ingesting streaming sales transactions
- Cleaning, validating, deduplicating
- Writing clean output to the **Silver layer**

ðŸ“ Defined inside:  
[`auto_ingestion_layer/01_streaming_to_silver`](real_time_ai_de_project/auto_ingestion_layer/01_streaming_to_silver.ipynb)

ðŸ“ Pipeline definition:  
[`dlt_pipeline/pl_streaming_data_to_silver`](dlt_pipeline)

---

> âœ… This fully automated workflow mimics how real-time pipelines are deployed at scale in top tech/data teams.





