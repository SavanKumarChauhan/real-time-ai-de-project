{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "545de8da-33e8-405b-8a0d-b4c70b979f58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Delete streaming data in Bronze container\n",
    "# dbutils.fs.rm(\"/mnt/realtimedeai/bronze/streaming/\", recurse=True)\n",
    "\n",
    "# # Delete transformed streaming data from Silver container\n",
    "# dbutils.fs.rm(\"/mnt/realtimedeai/silver/streaming/\", recurse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9a44160-b466-47c5-8ecc-f4699a618f92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.mkdirs(\"/mnt/realtimedeai/bronze/streaming/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a25cc4b5-37e3-4b4b-87b2-a6bb714b6b71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install faker "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03c460c5-fcd1-4081-8755-aee01f54fbe1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --------------------------------------\n",
    "# âœ… Required Libraries\n",
    "# --------------------------------------\n",
    "import time\n",
    "import pandas as pd\n",
    "from faker import Faker\n",
    "import random\n",
    "\n",
    "# --------------------------------------\n",
    "# âœ… Initialize Faker\n",
    "# --------------------------------------\n",
    "fake = Faker()\n",
    "\n",
    "# --------------------------------------\n",
    "# âœ… Load Batch Product & Store Data\n",
    "# --------------------------------------\n",
    "products_df = spark.read.parquet(\"/mnt/realtimedeai/bronze/batch/products/\").toPandas()\n",
    "stores_df = spark.read.parquet(\"/mnt/realtimedeai/bronze/batch/stores/\").toPandas()\n",
    "\n",
    "# Prepare lists from batch data (âœ” aligned)\n",
    "product_id_list = products_df['product_id'].tolist()\n",
    "product_price_map = products_df.set_index('product_id')['base_price'].to_dict()\n",
    "store_id_list = stores_df['store_id'].tolist()\n",
    "\n",
    "# --------------------------------------\n",
    "# âœ… Define Correct Streaming Data Generation\n",
    "# --------------------------------------\n",
    "def generate_streaming_sales(batch_size=5):\n",
    "    data = []\n",
    "    for _ in range(batch_size):\n",
    "        product_id = random.choice(product_id_list)\n",
    "        base_price = product_price_map[product_id]\n",
    "        store_id = random.choice(store_id_list)\n",
    "        quantity_sold = random.randint(1, 5)\n",
    "        sale_amount = round(base_price * quantity_sold, 2)\n",
    "\n",
    "        record = {\n",
    "            \"transaction_id\": fake.uuid4(),\n",
    "            \"product_id\": product_id,\n",
    "            \"store_id\": store_id,\n",
    "            \"quantity_sold\": quantity_sold,\n",
    "            \"sale_amount\": sale_amount,\n",
    "            \"transaction_time\": fake.iso8601()\n",
    "        }\n",
    "        data.append(record)\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# âœ… Preview one batch (for verification)\n",
    "display(generate_streaming_sales())\n",
    "\n",
    "# --------------------------------------\n",
    "# âœ… Define Paths Correctly\n",
    "# --------------------------------------\n",
    "streaming_path = \"/mnt/realtimedeai/bronze/streaming/\"\n",
    "local_path_prefix = \"/dbfs\"\n",
    "\n",
    "# # Ensure directory exists\n",
    "dbutils.fs.mkdirs(streaming_path)\n",
    "\n",
    "# # --------------------------------------\n",
    "# # âœ… Write Streaming Batches (Simulation)\n",
    "# # --------------------------------------\n",
    "num_batches = 10               # Adjust if needed\n",
    "batch_size = 5                 # Adjust if needed\n",
    "\n",
    "for batch_num in range(num_batches):\n",
    "    df = generate_streaming_sales(batch_size=batch_size)\n",
    "    file_path = f\"{local_path_prefix}{streaming_path}/sales_stream_batch_{batch_num}.parquet\"\n",
    "    df.to_parquet(file_path, index=False)\n",
    "    print(f\"âœ… Batch {batch_num + 1}/{num_batches} written to: {file_path}\")\n",
    "    time.sleep(5)  # Simulate streaming delay\n",
    "\n",
    "print(\"ðŸš€ Streaming data simulation completed successfully (fully aligned with batch product and store IDs)!\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_data_simulation_streaming",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
