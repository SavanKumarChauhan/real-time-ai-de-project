{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34517d63-8afd-4677-adc8-221829577a18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# dbutils.fs.rm(\"/mnt/realtimedeai/checkpoints/streaming/sales_transactions/\", recurse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a69d50b-87ec-4d32-8ec3-4ae47f700523",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType,LongType\n",
    "\n",
    "sales_schema = StructType([\n",
    "    StructField(\"transaction_id\", StringType(), True),\n",
    "    StructField(\"product_id\", LongType(), True),\n",
    "    StructField(\"store_id\", LongType(), True),\n",
    "    StructField(\"quantity_sold\", LongType(), True),\n",
    "    StructField(\"sale_amount\", DoubleType(), True),\n",
    "    StructField(\"transaction_time\", StringType(), True)\n",
    "])\n",
    "\n",
    "print(\"Schema defined successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f38e152-2d25-4cd5-a136-9788b4d97fc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "streaming_bronze_path = \"/mnt/realtimedeai/bronze/streaming/\"\n",
    "\n",
    "\n",
    "# Read Streaming Data \n",
    "\n",
    "streaming_sales_df = (spark\n",
    "    .readStream                                    \n",
    "    .format(\"parquet\")                             \n",
    "    .schema(sales_schema)                          \n",
    "    .load(streaming_bronze_path)                   \n",
    ")\n",
    "\n",
    "print(\"Streaming data source set up successfully!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4000e5ba-c988-4187-ae16-a7e7b9d5fd9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Deduplication: Remove duplicate transaction_id (streaming best practice)\n",
    "streaming_sales_cleaned_df = streaming_sales_df.dropDuplicates([\"transaction_id\"])\n",
    "\n",
    "\n",
    "print(\"Duplicates removed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16d2ea45-bd9f-4501-8155-c8f07813cfa8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Step 2: Null Handling & Invalid Data Filtering\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "streaming_sales_filtered_df = (\n",
    "    streaming_sales_cleaned_df\n",
    "    .filter(\n",
    "        (col(\"quantity_sold\") > 0) &\n",
    "        (col(\"sale_amount\") > 0) &\n",
    "        (col(\"transaction_id\").isNotNull()) &\n",
    "        (col(\"product_id\").isNotNull()) &\n",
    "        (col(\"store_id\").isNotNull())\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Nulls and invalid rows removed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54c7c8ab-d901-4711-929c-a50e1fc6bff4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_timestamp\n",
    "\n",
    "streaming_sales_timecasted_df = (\n",
    "    streaming_sales_filtered_df\n",
    "    .withColumn(\"transaction_time\", to_timestamp(col(\"transaction_time\"), \"yyyy-MM-dd'T'HH:mm:ss.SSSSSS\"))\n",
    ")\n",
    "\n",
    "print(\"transaction_time converted to TimestampType successfully (with correct format)!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "818986e7-e4eb-4bb1-84fb-705ab95ade03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Step 4: Add processed_time (ingestion timestamp)\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "\n",
    "streaming_sales_ready_df = (\n",
    "    streaming_sales_timecasted_df\n",
    "    .withColumn(\"processed_time\", current_timestamp())\n",
    ")\n",
    "\n",
    "print(\"processed_time column added successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e09fd7ac-21ee-4989-aa27-6b409813a2e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "silver_output_path = \"/mnt/realtimedeai/silver/streaming/sales_transactions/\"\n",
    "checkpoint_path = \"/mnt/realtimedeai/checkpoints/streaming/sales_transactions/\"\n",
    "\n",
    "\n",
    "write_query = (\n",
    "    streaming_sales_ready_df                       \n",
    "    .writeStream\n",
    "    .format(\"delta\")                               \n",
    "    .outputMode(\"append\")                          \n",
    "    .option(\"checkpointLocation\", checkpoint_path) \n",
    "    .start(silver_output_path)                     \n",
    ")\n",
    "\n",
    "print(\"Streaming write started successfully! Data is being written to the Silver layer.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0845484-d342-4897-aa86-bf260f59da8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "write_query.status\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10115147-c241-489d-8441-b250ba01f209",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "write_query.stop()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_streaming_ingestion_bronze_to_silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
